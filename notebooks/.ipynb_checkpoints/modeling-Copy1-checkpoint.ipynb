{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age (years)</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>mean diastolic bp (mmhg)</th>\n",
       "      <th>current gestational age</th>\n",
       "      <th>pregnancies (number)</th>\n",
       "      <th>first fasting glucose (mg/dl)</th>\n",
       "      <th>bmi pregestational (kg/m)</th>\n",
       "      <th>gestational dm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.5</td>\n",
       "      <td>12.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.3</td>\n",
       "      <td>33.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>21.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.5</td>\n",
       "      <td>12.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>21.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.5</td>\n",
       "      <td>16.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>81.3</td>\n",
       "      <td>55.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>30.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age (years)  ethnicity  mean diastolic bp (mmhg)  current gestational age  \\\n",
       "0           20        0.0                      73.5                     12.1   \n",
       "1           28        0.0                      75.5                     11.5   \n",
       "2           21        1.0                      70.5                     12.6   \n",
       "3           25        0.0                      92.5                     16.2   \n",
       "4           33        1.0                      79.0                     11.2   \n",
       "\n",
       "   pregnancies (number)  first fasting glucose (mg/dl)  \\\n",
       "0                   1.0                           81.3   \n",
       "1                   1.0                           76.0   \n",
       "2                   1.0                           86.0   \n",
       "3                   4.0                           81.3   \n",
       "4                   2.0                           86.0   \n",
       "\n",
       "   bmi pregestational (kg/m)  gestational dm  \n",
       "0                      33.20               0  \n",
       "1                      21.50               0  \n",
       "2                      21.41               0  \n",
       "3                      55.36               0  \n",
       "4                      30.85               1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_path='../data/processed/mean_impute_all_features.csv'\n",
    "dataframe = read_csv(full_path)\n",
    "dataframe.reset_index(drop=True, inplace=True)\n",
    "dataframe=dataframe.drop(['diabetes mellitus','mean systolic bp (mmhg)', 'central armellini fat (mm)'], axis=1)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X, y= dataframe.drop('gestational dm', axis=1), dataframe['gestational dm'] \n",
    "\n",
    "#Splitting X,y into Train & Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, stratify=y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of data distribution in the train set:\n",
      "86.79\n",
      "13.21\n",
      "-------------------\n",
      "Percentage of data distribution in the test set:\n",
      "85.19\n",
      "14.81\n"
     ]
    }
   ],
   "source": [
    "# Check if the labels are well shuffled in train and test set, to avoid imbalanced data\n",
    "a,b=np.bincount(y_train)\n",
    "print(f\"Percentage of data distribution in the train set:\")\n",
    "print(round(a/(a+b)*100,2))\n",
    "print(round(b/(a+b)*100,2))\n",
    "print(\"-------------------\")\n",
    "x,y=np.bincount(y_test)\n",
    "print(f\"Percentage of data distribution in the test set:\")\n",
    "print(round(x/(x+y)*100,2))\n",
    "print(round(y/(x+y)*100,2))\n",
    "# np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predicted labels for uniform strategy: [0 1]\n",
      "Test score:  0.4074074074074074\n",
      "Unique predicted labels for most_frequent strategy: [0]\n",
      "Test score:  0.8518518518518519\n",
      "Unique predicted labels for stratified strategy: [0 1]\n",
      "Test score:  0.7407407407407407\n",
      "Unique predicted labels for prior strategy: [0]\n",
      "Test score:  0.8518518518518519\n"
     ]
    }
   ],
   "source": [
    "# DummyClassifier to check accuracy for prediction of both classes 0,1\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score\n",
    "\n",
    "strategy = [\"uniform\", \"most_frequent\", \"stratified\", \"prior\"]\n",
    "for i in strategy:\n",
    "    dummy = DummyClassifier(strategy=i).fit(X_train, y_train)\n",
    "    dummy_pred = dummy.predict(X_test)\n",
    "\n",
    "    # checking unique labels\n",
    "    print(f'Unique predicted labels for {i} strategy: {np.unique(dummy_pred)}')\n",
    "\n",
    "    # checking accuracy\n",
    "    print('Test score: ', accuracy_score(y_test, dummy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88.88888888888889,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.88      1.00      0.94        23\\n           1       1.00      0.25      0.40         4\\n\\n    accuracy                           0.89        27\\n   macro avg       0.94      0.62      0.67        27\\nweighted avg       0.90      0.89      0.86        27\\n',\n",
       " DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=3,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=0, splitter='best'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def decisionTree(X_train, X_test, y_train, y_test):\n",
    "    # Train the model\n",
    "    tree = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    score = metrics.accuracy_score(y_test, y_pred) * 100\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return score, report, tree\n",
    "\n",
    "dt_score, dt_report, dt_tree = decisionTree(X_train, X_test, y_train, y_test)\n",
    "dt_score, dt_report, dt_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92.5925925925926,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.92      1.00      0.96        23\\n           1       1.00      0.50      0.67         4\\n\\n    accuracy                           0.93        27\\n   macro avg       0.96      0.75      0.81        27\\nweighted avg       0.93      0.93      0.92        27\\n',\n",
       " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                      metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                      weights='uniform'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def Knn_Classifier(X_train, X_test, y_train, y_test):\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = metrics.accuracy_score(y_test, y_pred) * 100\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return score, report, clf\n",
    "knn_score, knn_report, knn_tree = Knn_Classifier(X_train, X_test, y_train, y_test)\n",
    "knn_score, knn_report, knn_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "def conf_matrix(y_test, y_pred):\n",
    "    \"\"\"Function to generate confusion matrix and generate report\n",
    "       Arg: true labels and predicted labels array\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    f, ax = plt.subplots(figsize=(5,5))\n",
    "    sns.heatmap(cm,fmt=\".0f\", annot=True,linewidths=0.2, linecolor=\"purple\", ax=ax)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Grand Truth\")\n",
    "    plt.show()\n",
    "    TN = cm[0,0]\n",
    "    TP = cm[1,1]\n",
    "    FN = cm[1,0]\n",
    "    FP = cm[0,1]\n",
    "    Precision = TP/(TP+FP)\n",
    "    Recall = TP/(TP+FN)\n",
    "    F1_Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "    return pd.DataFrame([[Precision, Recall, F1_Score]],columns=[\"Precision\", \"Recall\", \"F1 Score\"], index=[\"Results\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Results</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Precision  Recall  F1 Score\n",
       "Results        1.0    0.25       0.4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check how Logistic Regression behaves with imbalanced data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# def logreg(X_train, X_test, y_train, y_test):\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, y_pred) * 100\n",
    "report = classification_report(y_test, y_pred)\n",
    "lr_score, lr_report, lr\n",
    "lr_report\n",
    "conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05325485,  0.4008576 , -0.02859898, -0.03892302,  0.0362747 ,\n",
       "         0.10581042,  0.08540639]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "pickle.dump(lr,open('model.pkl','wb') )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.58% risk of GDM\n",
      "86.87% risk of GDM\n"
     ]
    }
   ],
   "source": [
    "#Test #[age, ethni, bp, gage, preg, glu, bmi]\n",
    "Normal=[20, 0.0, 73, 12, 2, 87, 40]\n",
    "Pos=[33, 1, 79, 17, 5, 100, 50]\n",
    "B=np.array(Pos).reshape(1, -1)\n",
    "C=np.array(Normal).reshape(1, -1)\n",
    "loaded_model = pickle.load(open('model.pkl', 'rb'))\n",
    "result_pos = loaded_model.predict_proba(B)\n",
    "result_neg = loaded_model.predict_proba(C)\n",
    "\n",
    "\n",
    "x=np.round(result_pos[0,1]*100,2)\n",
    "y=np.round(result_neg[0,1]*100,2)\n",
    "print(f\"{y}% risk of GDM\")\n",
    "print(f\"{x}% risk of GDM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a balanced dataset\n",
    "The class weighing can be defined multiple ways; for example:\n",
    "\n",
    "1. Domain expertise, determined by talking to subject matter experts.\n",
    "2. Tuning, determined by a hyperparameter search such as a grid search.\n",
    "3. Heuristic, specified using a general best practice.\n",
    "\n",
    "A best practice for using the class weighting is to use the inverse of the class distribution present in the training dataset.Applying heuristic approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8a4a758df0c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m.80\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Predict on training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#Check how Logistic Regression behaves with class weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Modeling the data as is\n",
    "# Train model\n",
    "weight = {0:0.13, 1:.80}\n",
    "lr = LogisticRegression(solver='liblinear', class_weight=weight).fit(X_train, y_train)\n",
    " \n",
    "# Predict on training set\n",
    "lr_pred = lr.predict(X_test)\n",
    "print(\"Training set score: {:.3f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(lr.score(X_test, y_test)))\n",
    "# Checking accuracy\n",
    "print(accuracy_score(y_test, lr_pred))\n",
    "conf_matrix(y_test, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.899\n",
      "Test set score: 0.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logreg100 = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0d1ff356a94b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogreg100\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training set score: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg100\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test set score: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg100\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "logreg100 = LogisticRegression(C=100,solver='liblinear').fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Results</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Precision    Recall  F1 Score\n",
       "Results        0.4  0.333333  0.363636"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lr_pred = logreg100.predict(X_test)\n",
    "conf_matrix(y_test, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logreg100' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f8a45308b61e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlogreg100\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'logreg100' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/harmeetkaur/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.88888889, 0.88888889, 1.        , 1.        , 0.77777778,\n",
       "       0.77777778, 1.        , 0.88888889, 0.77777778, 0.875     ])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import pandas and numpy to handle and oraganize the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "imports from the scikit library what we need.  The first item is our Kfold module.\n",
    "Note also that we could have imported both kFold, train_test_split and cross_val_score \n",
    "all on the same line as \n",
    "\"from sklearn.model_selection\" using commas to seperate.  \n",
    "The point is just to clearly indicate the different modules\n",
    "we will be using, seperatly.  \n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "#Setting our KFold module for 5 splits:\n",
    "kf = KFold(n_splits=3,shuffle=True, random_state=123)\n",
    "#for our kf we said we want to split the data into 3 sets and run the test 3 times, Shuffle=True\n",
    "#will create our kfold sets randomly so that we don't introduce some bias into the data by taking the data points\n",
    "#in some regular order.  Lastly, we set a 'random_state' known most other places as a random seed so that we can reproduce\n",
    "#our random splits later if we choose too.\n",
    "\n",
    "#First we instantiate our model. In this case it is a Linear regression model:\n",
    "lr = LogisticRegression()\n",
    "\n",
    "#now we evaluate our model using the number of k-folds. Note that the 'cv' term is equal to our kf module we set above\n",
    "cv_scores = cross_val_score(lr,X_train,y_train,cv=kf)\n",
    "cv_scores\n",
    "\n",
    "'''\n",
    "Now we can see our 5 scores above.  Each k-fold times we run the model, we produce the score, discard that model, run a new model\n",
    "and rescore.  We repeat that for, in the above case, 5 times.  \n",
    "Below we will try splitting the data 10 times and running 10 models:\n",
    "'''\n",
    "\n",
    "kf = KFold(n_splits=10,shuffle=True, random_state=123)\n",
    "cv_scores = cross_val_score(lr,X_train,y_train,cv=kf)\n",
    "cv_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8875"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(6) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-349172048380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# apply SelectKBest class to extract top 10 best features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbestfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbestfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdfscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdfcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \"\"\"\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    755\u001b[0m                     estimator=estimator)\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n\u001b[0m\u001b[1;32m    758\u001b[0m                         dtype=None)\n\u001b[1;32m    759\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n",
      "\u001b[0;32m~/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0m\u001b[1;32m    152\u001b[0m                             \" a valid collection.\" % x)\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(6) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "# apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score'))  #print 10 best features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(6) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1cd3a532a44c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#use inbuilt class feature_importances of tree based classifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#plot graph of feature importances for better visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n",
      "\u001b[0;32m~/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0m\u001b[1;32m    152\u001b[0m                             \" a valid collection.\" % x)\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(6) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "from matplotlib import rcParams\n",
    "from matplotlib.cm import rainbow\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "rcParams['figure.figsize'] = 8,6\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh', color='grey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "correlated_features = set()\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "correlated_features\n",
    "X_uncor = X.drop(correlated_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad input shape ()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5be696cffef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrfecv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFECV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrfecv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_uncor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Optimal number of features: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfecv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0minstance\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroupKFold\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m         X, y = check_X_y(X, y, \"csr\", ensure_min_features=2,\n\u001b[0m\u001b[1;32m    493\u001b[0m                          force_all_finite=False)\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    758\u001b[0m                         dtype=None)\n\u001b[1;32m    759\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/insight/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: bad input shape ()"
     ]
    }
   ],
   "source": [
    "\n",
    "rfc = RandomForestClassifier(random_state=101)\n",
    "rfecv = RFECV(estimator=rfc, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "rfecv.fit(X_uncor, y)\n",
    "print('Optimal number of features: {}'.format(rfecv.n_features_))\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.title('Recursive Feature Elimination with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\n",
    "plt.xlabel('Number of features selected', fontsize=14, labelpad=20)\n",
    "plt.ylabel('% Correct Classification', fontsize=14, labelpad=20)\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, color='#303F9F', linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "print(np.where(rfecv.support_ == False)[0])\n",
    "# X.drop(X.columns[np.where(rfecv.support_ == False)[0]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09668249, 0.13152129, 0.19263033, 0.10952428, 0.08330941,\n",
       "       0.24070134, 0.14563086])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfecv.estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-dc973175e9d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_uncor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# dset['attr'] = X_uncor.columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# dset['importance'] = rfecv.estimator_.feature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
     ]
    }
   ],
   "source": [
    "dset = X_uncor(drop_index=True)\n",
    "dset\n",
    "# dset['attr'] = X_uncor.columns\n",
    "# dset['importance'] = rfecv.estimator_.feature_importances_\n",
    "\n",
    "# dset = dset.sort_values(by='importance', ascending=False)\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(16, 14))\n",
    "# plt.barh(y=dset['attr'], width=dset['importance'], color='#1976D2')\n",
    "# plt.title('RFECV - Feature Importances', fontsize=20, fontweight='bold', pad=20)\n",
    "# plt.xlabel('Importance', fontsize=14, labelpad=20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reset_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3085d5491b59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_uncor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_uncor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reset_index'"
     ]
    }
   ],
   "source": [
    "X_uncor=X_uncor.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('SVC', SVC()))\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('GNB', GaussianNB()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('GB', GradientBoostingClassifier()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c9b83a9893f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gestational dm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = df['gestational dm'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test accuracy and score\n",
    "names = []\n",
    "scores = []\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    scores.append(accuracy_score(y_test, y_pred))\n",
    "    names.append(name)\n",
    "tr_split = pd.DataFrame({'Name': names, 'Score': scores})\n",
    "print(tr_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-fold cross-validation\n",
    "names = []\n",
    "scores = []\n",
    "for name, model in models:\n",
    "    \n",
    "    kfold = KFold(n_splits=10, random_state=10) \n",
    "    score = cross_val_score(model, X, y, cv=kfold, scoring='accuracy').mean()\n",
    "    \n",
    "    names.append(name)\n",
    "    scores.append(score)\n",
    "kf_cross_val = pd.DataFrame({'Name': names, 'Score': scores})\n",
    "print(kf_cross_val)\n",
    "\n",
    "#Plot scores\n",
    "axis = sns.barplot(x = 'Name', y = 'Score', data = kf_cross_val)\n",
    "axis.set(xlabel='Classifier', ylabel='Accuracy')\n",
    "for p in axis.patches:\n",
    "    height = p.get_height()\n",
    "    axis.text(p.get_x() + p.get_width()/2, height + 0.005, '{:1.4f}'.format(height), ha=\"center\") \n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insight",
   "language": "python",
   "name": "insight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
